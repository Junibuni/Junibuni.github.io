---
title: Pytorch Einstein Summation Convention
date: 2024-11-04 11:20:00 +0900
categories: [AI, PyTorch]
tags: [pytorch, einstein notation, torch, matrix operations, deep learning]
use_math: true
---

## 아인슈타인 합산 법칙이란?

아인슈타인 합산 법칙(Einstein Summation Convention)은 벡터와 행렬 같은 다차원 배열의 연산을 간결하게 표현하는 데 사용된다. 토치(PyTorch)에서도 `torch.einsum` 함수를 통해 이 표기법을 활용해 복잡한 연산을 간단하게 구현할 수 있다. 이 방식으로 지표 인덱스를 자동으로 합산해, 반복되는 차원에 대해 더 간편하게 코드를 작성할 수 있게 된다.

여기서는 간단한 벡터와 행렬 연산부터 고급 연산까지 다양한 예제를 살펴보며 `torch.einsum`으로 구현하는 방법을 알아보자.

## 아인슈타인 표기법의 기본 개념

아인슈타인 표기법에서는 인덱스가 두 번 나타날 때 그 차원에 대해 자동으로 합산이 이루어진다. 예를 들어, 다음과 같은 식이 있다고 가정하자:

$$
A_{i} B_{i} = \sum_{i} A_{i} B_{i}
$$

여기서 인덱스 \(i\)가 중복되었으므로 해당 차원에 대한 합산이 자동으로 이루어진다. 이를 통해 코드에서 반복적인 합산 연산을 줄이고, 보다 직관적인 수식을 작성할 수 있다.

## 간단한 벡터 연산

### Dot Product

두 벡터 \(A\)와 \(B\)의 내적을 아인슈타인 표기법으로 다음과 같이 표현할 수 있다:

$$
C = \sum_{i} A_{i} B_{i}
$$

```python
import torch

A = torch.tensor([1, 2, 3])
B = torch.tensor([4, 5, 6])
result = torch.einsum('i,i->', A, B)
print(result)  # 출력: tensor(32)
```

## 복잡한 행렬 연산

행렬 간의 복잡한 연산도 아인슈타인 표기법으로 쉽게 구현할 수 있다. 이를 통해 전치, 합산, 곱셈 등을 명시적으로 수행할 수 있다.

### Transpose

행렬 \(M\)의 전치는 다음과 같이 표현할 수 있다:

$$
M^T_{ji} = M_{ij}
$$

```python
matrix = torch.tensor([[1, 2], [3, 4]])
transposed = torch.einsum('ij->ji', matrix)
print(transposed)  # 출력: tensor([[1, 3], [2, 4]])
```

### Sum

행렬의 모든 원소를 합산하려면 다음과 같이 표기할 수 있다:

$$
\sum_{i,j} M_{ij}
$$

```python
total_sum = torch.einsum('ij->', matrix)
print(total_sum)  # 출력: tensor(10)
```

### Column Sum

행렬의 각 열을 합산하는 식은 다음과 같다:

$$
\sum_{j} M_{ij}
$$

```python
column_sum = torch.einsum('ij->i', matrix)
print(column_sum)  # 출력: tensor([3, 7])
```

### Row Sum

행렬의 각 행을 합산하는 식은 다음과 같다:

$$
\sum_{i} M_{ij}
$$

```python
row_sum = torch.einsum('ij->j', matrix)
print(row_sum)  # 출력: tensor([4, 6])
```

## Matrix-Matrix Multiplication

행렬 곱셈은 아인슈타인 표기법에서 인덱스를 공유하는 방식으로 작성된다.

$$
C_{ij} = \sum_{k} A_{ik} B_{kj}
$$

```python
A = torch.tensor([[1, 2], [3, 4]])
B = torch.tensor([[5, 6], [7, 8]])
product = torch.einsum('ik,kj->ij', A, B)
print(product)
# 출력: tensor([[19, 22], [43, 50]])
```

## Dot Product, Outer Product, Hadamard Product

### Dot Product

위에서 설명한 벡터 내적을 다시 확인해보자:

$$
C = \sum_{i} A_{i} B_{i}
$$

```python
dot_product = torch.einsum('i,i->', A, B)
```

### Outer Product

두 벡터의 외적은 다음과 같은 수식으로 표현된다:

$$
C_{ij} = A_{i} B_{j}
$$

```python
outer_product = torch.einsum('i,j->ij', A, B)
print(outer_product)
# 출력: tensor([[ 4,  5,  6],
#              [ 8, 10, 12],
#              [12, 15, 18]])
```

### Hadamard Product

아다마르 곱은 두 행렬의 원소별 곱이다:

$$
C_{ij} = A_{ij} \cdot B_{ij}
$$

```python
hadamard_product = torch.einsum('ij,ij->ij', A, B)
print(hadamard_product)
```

## Batch Matrix Multiplication

배치 행렬 곱셈은 다음과 같이 표현할 수 있다:

$$
C_{bik} = \sum_{j} A_{bij} B_{bjk}
$$

```python
A = torch.randn(2, 3, 4)  # 2개의 3x4 행렬
B = torch.randn(2, 4, 5)  # 2개의 4x5 행렬
batch_product = torch.einsum('bij,bjk->bik', A, B)
print(batch_product.shape)  # 출력: torch.Size([2, 3, 5])
```

## Bilinear Transformation

이중선형변환은 다음 수식과 같이 두 벡터 사이에 행렬 \(W\)를 곱하는 연산이다:

$$
C = \sum_{i,j} A_{i} W_{ij} B_{j}
$$

```python
A = torch.randn(3)
B = torch.randn(4)
W = torch.randn(3, 4, 5)
bilinear_result = torch.einsum('i,ij,j->', A, W, B)
print(bilinear_result)
```

## 결론

`torch.einsum`을 통해 아인슈타인 표기법을 활용하면 복잡한 행렬 연산을 직관적이고 간단하게 표현할 수 있다. 이를 통해 코드의 가독성이 크게 향상되고, 다양한 벡터 및 행렬 연산을 쉽게 구현할 수 있다. 특히 고차원 데이터와 딥러닝 연산에서 유용하게 활용할 수 있는 기능이다.

## 참고자료

- *<https://pytorch.org/docs/stable/generated/torch.einsum.html>*