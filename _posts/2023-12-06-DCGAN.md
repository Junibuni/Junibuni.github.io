---
title: DCGAN 이론
date: 2023-12-06 23:56:00 +0800
categories: [AI, GAN]
tags: [dcgan, deep learning, cnn, image generation, neural network, ml, ai, gan, image synthesis, computer vision, dnn, image processing, python, pytorch]
use_math: true
---
# 생성모델
컴퓨터는 어떻게 존재하지 않는 그럴싸한 이미지를 만들어 낼 수 있을까?

## 이미지 데이터
이미지 데이터는 다차원 특징 공간의 한 점으로 표현 된다. 사람들의 키 분포도가 아래와 같이 주어졌을 때, 특정 사람의 키는 1차원 공간의 점 (키)로 표현 될 수 있다. 당연한 얘기지만, 이미지 데이터와 같이 다차원 공간에 존재하는 데이터도 마찬가지라는 얘기다. 따라서 이미지의 분포를 근사하는 모델을 생성한다면, 그럴싸한 데이터를 생성할 수 있다는 얘기이다!
<img src="{{page.img_pth}}height_distribution.png">

정리하자면, 생성모델은 다음과 같이 설명할 수 있다.
- 결합확률분포 (joint probability distribution) 의 통계모델
- 새로운 인스턴스 (new data instances)를 생성하는 아키텍쳐

## 생성모델의 목표


이미지 생성모델의 목표라고 하면, 이미지 데이터의 분포를 근사하는 모델, G (Generator) 를 만드는 것이 생성모델의 목표이다. 이때, 모델 G 가 원래 이미지의 분포를 잘 모델링 할 수 있다면 잘 동작한다고 볼 수 있다.

# Generative Adversarial Network (GAN)
GAN은 이름에서 볼 수 있듯이, 적대적인(Adversarial) 네트워크이다. 이게 무슨 뜻인가 하면, 두개의 네트워크를 함께 학습한다는 뜻이다. 두가지 모델은 생성자(Generator)와 판별자(Discriminator)로 이루어져 있고, inference 과정에서는 생성자 모델만 쓰여 새로운 이미지를 생성하게 된다. 이 두가지 모델을 함께 학습시키면서 생성모델을 학습할 수 있게 된다. 생성자는 다음과 같은 목적함수(objective funtion)을 통해 이미지 분포를 학습 할 수 있다.

[GAN 문헌](https://arxiv.org/abs/1406.2661)

\\[ 
\min_{G} \max_{D} V(D,G) = E_{x \sim p_{data}(x)}[\log{D(x)}] + E_{z \sim p_{z}(z)}[\log{1-D(G(z))}]
\\]

위에서 \\(G\\)는 목적함수를 최대한 낮추고자 노력하고, \\(D\\)는 목적함수를 최대로 높이고자 노력하게 된다. 다시말해 우변의 첫번째 항은 원본 데이터에 대해서는 \\(D(x)\\)가 1을 뱉어낼 수 있게 하고, 두번째 항은 가짜 이미지에 대해서 0을 뱉어낼 수 있도록 학습하겠다는 뜻이다.

<img src="{{page.img_pth}}GAN_structure.png">

GAN은 위와 같이 학습을 진행하고, Discriminator를 통해 나온 값으로 Fake(0)와 Real(1)을 판별한다. 이 결과를 통해 Loss 값을 구하게 되고, 각 모델은 다음과 같이 업데이트 될 수 있다.

Generator는 목적함수를 낮춰야 하기 때문에 -ve 방향으로 업데이트:

\\[
\theta_G \leftarrow \theta_G - \eta \cdot \nabla_{\theta_G} \left( \frac{1}{m} \sum_{i=1}^{m} \log(D(G(z_i))) \right)
\\]

Discriminator는 Fake(0), Real(1)이 되는 방향으로 gradient를 타고 올라갈 수 있게 업데이트:

\\[
\theta_D \leftarrow \theta_D + \eta \cdot \nabla_{\theta_D} \left( \frac{1}{m} \sum_{i=1}^{m} \left[ \log(D(x_i)) + \log(1 - D(G(z_i))) \right] \right)
\\]

하게 된다.

동일한 목적함수 식에 대해서 \\(D\\)와 \\(G\\)는 서로 다른 목적을 가지기 때문에, **min-max**게임 이론에 기반하는 optimization문제로 볼 수 있다. 이런식으로 동일한 식으로 \\(G\\)는 minimize 하고 \\(D\\)는 maximize하는 방향으로 학습하면, 생성자는 "그럴싸한"이미지를 생성해 낼 수 있는 모델이 될 수 있다는게 문헌에서 주장하는 이론이다.

## GAN의 수렴 과정
<img src="{{page.img_pth}}GAN_schematic.png">